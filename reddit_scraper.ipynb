{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jgber\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jgber\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jgber\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jgber\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jgber\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jgber\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Reddit posts for query: terminal illness\n",
      "Fetched 100 posts so far...\n",
      "Fetched 200 posts so far...\n",
      "Fetched 249 posts so far...\n",
      "Saved 249 posts to reddit_terminal_illness_posts.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def fetch_reddit_posts(query, after=None, limit=100):\n",
    "    \"\"\"\n",
    "    Fetch a batch of Reddit posts matching the query using Reddit's public JSON search endpoint.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        after (str): The \"after\" token for pagination (if any).\n",
    "        limit (int): Number of posts to return (maximum allowed is typically 100).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of posts, after token) where each post is a dict of post data.\n",
    "    \"\"\"\n",
    "    url = \"https://www.reddit.com/search.json\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; RedditScraper/1.0)\"\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"sort\": \"new\",    # Fetch newest posts first\n",
    "        \"limit\": limit,\n",
    "        \"after\": after\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            return [], None\n",
    "        data = response.json().get(\"data\", {})\n",
    "        posts = data.get(\"children\", [])\n",
    "        after = data.get(\"after\", None)\n",
    "        return posts, after\n",
    "    except Exception as e:\n",
    "        print(\"Exception during fetch:\", e)\n",
    "        return [], None\n",
    "\n",
    "def scrape_reddit(query=\"terminal illness\", max_posts=500):\n",
    "    \"\"\"\n",
    "    Scrape historical Reddit posts containing the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query (default is \"terminal illness\").\n",
    "        max_posts (int): The maximum number of posts to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of post data dictionaries.\n",
    "    \"\"\"\n",
    "    all_posts = []\n",
    "    after = None  # Token for pagination\n",
    "    fetched_count = 0\n",
    "\n",
    "    while fetched_count < max_posts:\n",
    "        posts_batch, after = fetch_reddit_posts(query, after=after, limit=100)\n",
    "        if not posts_batch:\n",
    "            print(\"No more posts returned by the API.\")\n",
    "            break\n",
    "\n",
    "        for post in posts_batch:\n",
    "            all_posts.append(post[\"data\"])\n",
    "            fetched_count += 1\n",
    "            if fetched_count >= max_posts:\n",
    "                break\n",
    "\n",
    "        print(f\"Fetched {fetched_count} posts so far...\")\n",
    "        if after is None:\n",
    "            # No more pages available\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Pause to respect rate limits\n",
    "\n",
    "    return all_posts\n",
    "\n",
    "def save_posts_to_csv(posts, filename=\"reddit_terminal_illness_posts.csv\"):\n",
    "    \"\"\"\n",
    "    Save the list of Reddit posts to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        posts (list): List of post data dictionaries.\n",
    "        filename (str): The output CSV file name.\n",
    "    \"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to save.\")\n",
    "        return\n",
    "\n",
    "    # Define the CSV columns; adjust fields as needed.\n",
    "    headers = [\n",
    "        \"id\", \"title\", \"selftext\", \"subreddit\", \"author\",\n",
    "        \"created_utc\", \"url\", \"score\", \"num_comments\"\n",
    "    ]\n",
    "    try:\n",
    "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            for post in posts:\n",
    "                row = {field: post.get(field, \"\") for field in headers}\n",
    "                writer.writerow(row)\n",
    "        print(f\"Saved {len(posts)} posts to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving CSV:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the search query (space will be URL-encoded automatically by requests).\n",
    "    query = \"terminal illness\"\n",
    "    print(f\"Scraping Reddit posts for query: {query}\")\n",
    "    posts = scrape_reddit(query=query, max_posts=500)\n",
    "    save_posts_to_csv(posts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
